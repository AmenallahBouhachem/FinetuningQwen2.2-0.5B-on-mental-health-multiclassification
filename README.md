# Qwen Model Fine-Tuning and Evaluation

This project contains Jupyter notebooks and resources for fine-tuning and evaluating Qwen language models, including quantized and distilled variants. The workflows leverage various techniques and tools for efficient training and assessment.

## Contents

- `finetune_qwen.ipynb`: Notebook for fine-tuning the Qwen model.
- `Finetune_DeepSeek_R1_Distill_Qwen_1_5B.ipynb`: Fine-tuning Qwen 1.5B using DeepSeek R1 distillation.
- `Finetune_Qwen2_5_3B_Using_Unsloth.ipynb`: Fine-tuning Qwen2 5.3B with Unsloth for efficient training.
- `QwenQuantizedEvalution.ipynb`: Evaluation of quantized Qwen models.
- `Basline_eval_qwen.txt`: Baseline evaluation results for Qwen models.

## Getting Started

1. Clone this repository and open the notebooks in Jupyter or VS Code.
2. Follow the instructions in each notebook to fine-tune or evaluate the models.
3. Review `Basline_eval_qwen.txt` for baseline results.

## Requirements
- Python 3.8+
- Jupyter Notebook
- PyTorch, Transformers, and other dependencies as specified in the notebooks

## Notes
- Ensure you have sufficient GPU resources for training large models.
- Refer to each notebook for specific setup and usage instructions.

## License
This project is for research and educational purposes.
